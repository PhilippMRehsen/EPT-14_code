{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/preprocessing/process_biodiscover.py \\\n",
    "    --csv_path ../EPT_final_file_species_level.csv \\\n",
    "    --out_folder data/processed/ept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/01_train_test_split.py \\\n",
    "    --csv_path \"data/processed/ept/01_EPT_processed.csv\" \\\n",
    "    --target_col \"Specimen Weight\" \\\n",
    "    --group_col \"individual\" \\\n",
    "    --n_splits 5 \\\n",
    "    --out_folder \"data/processed/ept-biomass\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/processed/ept-biomass/01_EPT_processed_5splits_Specimen Weight.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/preprocessing/test_dataset_config.py \\\n",
    "    --data_folder \"../0_images/\" \\\n",
    "    --dataset_config \"conf/user_datasets.py\" \\\n",
    "    --dataset_name \"biodiscover\" \\\n",
    "    --csv_path \"data/processed/ept-biomass/01_EPT_processed_5splits_Specimen Weight.csv\" \\\n",
    "    --label \"Specimen Weight\" \\\n",
    "    --fold 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mphilipp-rehsen\u001b[0m (\u001b[33muni-due\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login() #581221dff0063a897a4eff9d285fa871a2c8e7f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use for testing. Use all folds for actual results\n",
    "!python scripts/02_train.py \\\n",
    "    --data_folder \"../0_images/\" \\\n",
    "    --dataset_config \"conf/user_datasets.py\" \\\n",
    "    --dataset_name \"biodiscover\" \\\n",
    "    --csv_path \"data/processed/ept-biomass/01_EPT_processed_5splits_Specimen Weight.csv\" \\\n",
    "    --label \"Specimen Weight\" \\\n",
    "    --fold 0 \\\n",
    "    --class_map \"none\" \\\n",
    "    --imsize 224 \\\n",
    "    --batch_size 256 \\\n",
    "    --aug 'flips-rotate-keep-aspect' \\\n",
    "    --load_to_memory 'False' \\\n",
    "    --model 'efficientnet_b0' \\\n",
    "    --opt 'adamw' \\\n",
    "    --max_epochs 10 \\\n",
    "    --min_epochs 0 \\\n",
    "    --early_stopping 'False' \\\n",
    "    --early_stopping_patience 50 \\\n",
    "    --criterion 'l1' \\\n",
    "    --lr 0.004 \\\n",
    "    --auto_lr 'True' \\\n",
    "    --log_dir 'tests' \\\n",
    "    --out_folder 'outputs' \\\n",
    "    --out_prefix '00_ept-biomass_test_run' \\\n",
    "    --deterministic 'True' \\\n",
    "    --precision '16-mixed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.8 (you have 1.4.7). Upgrade using: pip install --upgrade albumentations\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/efficientnet_b0.ra_in1k)\n",
      "INFO:timm.models._hub:[timm/efficientnet_b0.ra_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mphilipp-rehsen\u001b[0m (\u001b[33muni-due\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240531_082215-240531-0822-ed48\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m_efficientnet_b0_f0_240531-0822-ed48\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/uni-due/lightning_logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/uni-due/lightning_logs/runs/240531-0822-ed48\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "INFO: GPU available: False, used: False\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: `Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "INFO: `Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "INFO: `Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /tf/notebooks/EPT/1_taxonomist/biodiscover/_efficientnet_b0/f0 exists and is not empty.\n",
      "INFO: \n",
      "  | Name  | Type  | Params | In sizes         | Out sizes\n",
      "---------------------------------------------------------------\n",
      "0 | model | Model | 4.0 M  | [1, 3, 224, 224] | [1, 1]   \n",
      "---------------------------------------------------------------\n",
      "4.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 M     Total params\n",
      "16.035    Total estimated model params size (MB)\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name  | Type  | Params | In sizes         | Out sizes\n",
      "---------------------------------------------------------------\n",
      "0 | model | Model | 4.0 M  | [1, 3, 224, 224] | [1, 1]   \n",
      "---------------------------------------------------------------\n",
      "4.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 M     Total params\n",
      "16.035    Total estimated model params size (MB)\n",
      "Epoch 0:   3%|‚ñå                  | 10/362 [02:53<1:42:00,  0.06it/s, v_num=ed48]^C\n",
      "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:156: `.test(ckpt_path=\"best\")` is called with Trainer configured with multiple `ModelCheckpoint` callbacks. It will use the best checkpoint path from first checkpoint callback.\n",
      "Traceback (most recent call last):\n",
      "  File \"scripts/02_train.py\", line 29, in <module>\n",
      "    trainer = main(\n",
      "  File \"scripts/02_train.py\", line 23, in main\n",
      "    return src.TaxonomistModel(\n",
      "  File \"/tf/notebooks/EPT/1_taxonomist/src/taxonomist/taxonomist_model.py\", line 461, in train_model\n",
      "    self._perform_training(trainer, model, dm, resume_ckpt)\n",
      "  File \"/tf/notebooks/EPT/1_taxonomist/src/taxonomist/taxonomist_model.py\", line 346, in _perform_training\n",
      "    trainer.test(model, datamodule=dm, ckpt_path=\"best\")\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py\", line 754, in test\n",
      "    return call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py\", line 791, in _test_impl\n",
      "    ckpt_path = self._checkpoint_connector._select_ckpt_path(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py\", line 104, in _select_ckpt_path\n",
      "    ckpt_path = self._parse_ckpt_path(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py\", line 171, in _parse_ckpt_path\n",
      "    raise ValueError(\n",
      "ValueError: `.test(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.\n",
      "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.8 (you have 1.4.7). Upgrade using: pip install --upgrade albumentations\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/efficientnet_b0.ra_in1k)\n",
      "INFO:timm.models._hub:[timm/efficientnet_b0.ra_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mphilipp-rehsen\u001b[0m (\u001b[33muni-due\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240531_082556-240531-0825-f056\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m_efficientnet_b0_f1_240531-0825-f056\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/uni-due/lightning_logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/uni-due/lightning_logs/runs/240531-0825-f056\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "INFO: GPU available: False, used: False\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: `Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "INFO: `Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "INFO: `Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:156: `.test(ckpt_path=\"best\")` is called with Trainer configured with multiple `ModelCheckpoint` callbacks. It will use the best checkpoint path from first checkpoint callback.\n",
      "Traceback (most recent call last):\n",
      "  File \"scripts/02_train.py\", line 29, in <module>\n",
      "    trainer = main(\n",
      "  File \"scripts/02_train.py\", line 23, in main\n",
      "    return src.TaxonomistModel(\n",
      "  File \"/tf/notebooks/EPT/1_taxonomist/src/taxonomist/taxonomist_model.py\", line 461, in train_model\n",
      "    self._perform_training(trainer, model, dm, resume_ckpt)\n",
      "  File \"/tf/notebooks/EPT/1_taxonomist/src/taxonomist/taxonomist_model.py\", line 346, in _perform_training\n",
      "    trainer.test(model, datamodule=dm, ckpt_path=\"best\")\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py\", line 754, in test\n",
      "    return call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/trainer.py\", line 791, in _test_impl\n",
      "    ckpt_path = self._checkpoint_connector._select_ckpt_path(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py\", line 104, in _select_ckpt_path\n",
      "    ckpt_path = self._parse_ckpt_path(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py\", line 171, in _parse_ckpt_path\n",
      "    raise ValueError(\n",
      "ValueError: `.test(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.\n",
      "^C\n",
      "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.8 (you have 1.4.7). Upgrade using: pip install --upgrade albumentations\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/efficientnet_b0.ra_in1k)\n",
      "INFO:timm.models._hub:[timm/efficientnet_b0.ra_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "None\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"scripts/02_train.py\", line 29, in <module>\n",
      "    trainer = main(\n",
      "  File \"scripts/02_train.py\", line 23, in main\n",
      "    return src.TaxonomistModel(\n",
      "  File \"/tf/notebooks/EPT/1_taxonomist/src/taxonomist/taxonomist_model.py\", line 449, in train_model\n",
      "    logger = self._create_logger(model)\n",
      "  File \"/tf/notebooks/EPT/1_taxonomist/src/taxonomist/taxonomist_model.py\", line 296, in _create_logger\n",
      "    logger.watch(model)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loggers/wandb.py\", line 420, in watch\n",
      "    self.experiment.watch(model, log=log, log_freq=log_freq, log_graph=log_graph)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/lightning/fabric/loggers/logger.py\", line 118, in experiment\n",
      "    return fn(self)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loggers/wandb.py\", line 406, in experiment\n",
      "    self._experiment = wandb.init(**self._wandb_init)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/wandb_init.py\", line 1163, in init\n",
      "    wi.setup(kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/wandb_init.py\", line 300, in setup\n",
      "    wandb_login._login(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/wandb_login.py\", line 325, in _login\n",
      "    logged_in = wlogin.login()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/wandb_login.py\", line 175, in login\n",
      "    self.login_display()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/wandb_login.py\", line 180, in login_display\n",
      "    username = self._wl._get_username()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/wandb_setup.py\", line 194, in _get_username\n",
      "    self._load_viewer()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/wandb_setup.py\", line 216, in _load_viewer\n",
      "    s.query_with_timeout()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/lib/server.py\", line 35, in query_with_timeout\n",
      "    viewer_tuple, viewer_thread = async_viewer()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/util.py\", line 1188, in wrapper\n",
      "    result = q.get(True, timeout)\n",
      "  File \"/usr/lib/python3.8/queue.py\", line 179, in get\n",
      "    self.not_empty.wait(remaining)\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 306, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "KeyboardInterrupt\n",
      "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.8 (you have 1.4.7). Upgrade using: pip install --upgrade albumentations\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/efficientnet_b0.ra_in1k)\n",
      "INFO:timm.models._hub:[timm/efficientnet_b0.ra_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mphilipp-rehsen\u001b[0m (\u001b[33muni-due\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"scripts/02_train.py\", line 29, in <module>\n",
      "    trainer = main(\n",
      "  File \"scripts/02_train.py\", line 23, in main\n",
      "    return src.TaxonomistModel(\n",
      "  File \"/tf/notebooks/EPT/1_taxonomist/src/taxonomist/taxonomist_model.py\", line 449, in train_model\n",
      "    logger = self._create_logger(model)\n",
      "  File \"/tf/notebooks/EPT/1_taxonomist/src/taxonomist/taxonomist_model.py\", line 296, in _create_logger\n",
      "    logger.watch(model)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loggers/wandb.py\", line 420, in watch\n",
      "    self.experiment.watch(model, log=log, log_freq=log_freq, log_graph=log_graph)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/lightning/fabric/loggers/logger.py\", line 118, in experiment\n",
      "    return fn(self)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/lightning/pytorch/loggers/wandb.py\", line 406, in experiment\n",
      "    self._experiment = wandb.init(**self._wandb_init)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/wandb_init.py\", line 1164, in init\n",
      "    return wi.init()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/wandb_init.py\", line 793, in init\n",
      "    run._on_init()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/wandb_run.py\", line 2400, in _on_init\n",
      "    version_result = version_handle.wait(timeout=30)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/lib/mailbox.py\", line 283, in wait\n",
      "    found, abandoned = self._slot._get_and_clear(timeout=wait_timeout)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/lib/mailbox.py\", line 130, in _get_and_clear\n",
      "    if self._wait(timeout=timeout):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/lib/mailbox.py\", line 126, in _wait\n",
      "    return self._event.wait(timeout=timeout)\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 558, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 306, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "for fold in range(5):\n",
    "    !python scripts/02_train.py \\\n",
    "        --data_folder \"../0_images/\" \\\n",
    "        --dataset_config \"conf/user_datasets.py\" \\\n",
    "        --dataset_name \"biodiscover\" \\\n",
    "        --csv_path \"data/processed/ept-biomass/01_EPT_processed_5splits_Specimen Weight.csv\" \\\n",
    "        --label \"Specimen Weight\" \\\n",
    "        --fold {fold} \\\n",
    "        --class_map 'none' \\\n",
    "        --imsize 224 \\\n",
    "        --batch_size 256 \\\n",
    "        --aug 'flips-rotate-keep-aspect' \\\n",
    "        --load_to_memory 'False' \\\n",
    "        --model 'efficientnet_b0' \\\n",
    "        --opt 'adamw' \\\n",
    "        --max_epochs 50 \\\n",
    "        --min_epochs 0 \\\n",
    "        --early_stopping 'False' \\\n",
    "        --early_stopping_patience 50 \\\n",
    "        --criterion 'l1' \\\n",
    "        --lr 4.786300923226383e-07 \\\n",
    "        --auto_lr 'False' \\\n",
    "        --log_dir 'ept-biomass' \\\n",
    "        --out_folder 'outputs' \\\n",
    "        --out_prefix '01_ept-biomass_run' \\\n",
    "        --deterministic 'True' \\\n",
    "        --precision '16-mixed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This just sets the checkpoint as the first (best) model in the directory above, as the unique identifier is always different.\n",
    "!python scripts/03_predict.py \\\n",
    "    --data_folder \"../0_images/\" \\\n",
    "    --dataset_config \"conf/user_datasets.py\" \\\n",
    "    --dataset_name \"biodiscover\" \\\n",
    "    --csv_path \"data/processed/ept/01_EPT_processed_5splits_Final_species_label.csv\" \\\n",
    "    --label \"Specimen Weight\" \\\n",
    "    --fold 0 \\\n",
    "    --class_map \"none\" \\\n",
    "    --imsize 224 \\\n",
    "    --batch_size 1024 \\\n",
    "    --aug 'none' \\\n",
    "    --out_folder 'outputs' \\\n",
    "    --tta 'False' \\\n",
    "    --out_prefix '' \\\n",
    "    --ckpt_path 'outputs/biodiscover/00_ept-biomass_test_run_efficientnet_b0/f0/00_ept-biomass_test_run_efficientnet_b0_f0_240529-1243-36c0_epoch09_val-loss1.85.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/04_group_predictions.py \\\n",
    "    --predictions 'outputs/biodiscover/00_ept-biomass_test_run_efficientnet_b0/f0/predictions/biodiscover_none/_00_ept-biomass_test_run_efficientnet_b0_f0_240529-1243-36c0_epoch09_val-loss1.85_none.csv' \\\n",
    "    --reference_csv \"data/processed/ept/01_EPT_processed_5splits_Final_species_label.csv\" \\\n",
    "    --reference_target \"Specimen Weight\" \\\n",
    "    --fold 0 \\\n",
    "    --reference_group \"individual\" \\\n",
    "    --agg_func \"median\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "def confusion_matrixX(y_true, \n",
    "                      y_pred, \n",
    "                      classes, \n",
    "                      figsize=(15,15), \n",
    "                      fonts=(18,10,18), \n",
    "                      rotate=False,\n",
    "                      grid=False,\n",
    "                      normalize='true'):\n",
    "\n",
    "    # Check the inputs\n",
    "    s_true = set(np.unique(y_true))\n",
    "    s_pred = set(np.unique(y_pred))\n",
    "\n",
    "    if classes == 'union':\n",
    "        classes = sorted(list(s_true | s_pred))\n",
    "\n",
    "    if len((s_true | s_pred) - set(classes)) > 0:\n",
    "        warnings.warn(f\"The inputs contain classes not present in the class list: {(s_true | s_pred) - set(classes)}\")\n",
    "\n",
    "    cm = confusion_matrix(y_true, \n",
    "                          y_pred, \n",
    "                          labels=classes,\n",
    "                          normalize=normalize)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm*100\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    mask = np.zeros_like(cm)\n",
    "    mask[cm==0] = 1\n",
    "    sns.heatmap(cm, \n",
    "                annot=True, \n",
    "                annot_kws={\"size\": fonts[0]},\n",
    "                fmt='.0f', \n",
    "                cmap=\"YlGnBu\",\n",
    "                xticklabels=classes, \n",
    "                yticklabels=classes,\n",
    "                mask=mask,\n",
    "                square=True, \n",
    "                cbar=False,\n",
    "                ax=ax)\n",
    "    \n",
    "    if grid:\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.yaxis.grid(color='gray', linestyle='dashed')\n",
    "        ax.xaxis.grid(color='gray', linestyle='dashed')\n",
    "\n",
    "    ax.set_ylabel('True label', size=fonts[2])\n",
    "    ax.set_xlabel('Predicted label', size=fonts[2])\n",
    "    ax.yaxis.set_ticks_position('none') \n",
    "    ax.xaxis.set_ticks_position('none') \n",
    "    \n",
    "    if rotate:\n",
    "        if isinstance(rotate, list):\n",
    "            rot_x = rotate[0]\n",
    "            rot_y = rotate[1]\n",
    "        else:\n",
    "            rot_x = rot_y = rotate\n",
    "        plt.setp(ax.get_yticklabels(), rotation=rot_x, ha=\"right\", rotation_mode=\"anchor\", size=fonts[1])\n",
    "        plt.setp(ax.get_xticklabels(), rotation=rot_y, ha=\"right\", rotation_mode=\"anchor\", size=fonts[1])\n",
    "    else:\n",
    "        plt.setp(ax.get_yticklabels(), size=fonts[1])\n",
    "        plt.setp(ax.get_xticklabels(), size=fonts[1])\n",
    "    plt.show()\n",
    "        \n",
    "from sklearn.metrics import classification_report\n",
    "def classification_reportX(*args, \n",
    "                           figsize=None, \n",
    "                           fonts=(12,10,10),\n",
    "                           rotate=True, \n",
    "                           bbox_anchor=(1.15,0.8), \n",
    "                           **kwargs):\n",
    "    \"\"\"Extends classification report by adding an useful plot for the performance across classes\"\"\"\n",
    "    r = classification_report(*args, **kwargs, output_dict=True)\n",
    "    rdf0 = pd.DataFrame(r)\n",
    "    rdf = rdf0.T.iloc[:-3,:].sort_values('support',ascending=False)\n",
    "    fig, ax1 = plt.subplots(figsize=figsize)\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    xbar = range(len(rdf))\n",
    "    ax1.bar(xbar,rdf['support'], alpha=0.2)\n",
    "    ax2.plot(rdf['f1-score'], 'ro', label='f1-score')\n",
    "    ax2.plot(rdf['precision'], 'g*', label='precision', alpha=0.5)\n",
    "    ax2.plot(rdf['recall'], 'b*', label='recall', alpha=0.5)\n",
    "\n",
    "    ax2.vlines(xbar, rdf['f1-score'], rdf['precision'], 'g')\n",
    "    ax2.vlines(xbar, rdf['f1-score'], rdf['recall'], 'b')\n",
    "\n",
    "    ax2.hlines(rdf0['weighted avg']['f1-score'], 0,len(rdf), color='r', linestyle='--', label='weighted f1-score')\n",
    "    ax2.hlines(rdf0['macro avg']['f1-score'], 0,len(rdf), color='r', linestyle='-.', label='macro f1-score')\n",
    "    ax2.hlines(rdf0['accuracy']['recall'], 0,len(rdf), color='b', linestyle='--', label='accuracy')\n",
    "    \n",
    "    if rotate:\n",
    "        plt.setp(ax1.get_xticklabels(), \n",
    "                 rotation=45, \n",
    "                 ha=\"right\", \n",
    "                 rotation_mode=\"anchor\",\n",
    "                 size=fonts[1])\n",
    "\n",
    "    fig.legend(bbox_to_anchor=bbox_anchor,\n",
    "               prop={\"size\": fonts[2]})\n",
    "    \n",
    "    ax2.set_ylabel(\"Score\", size=fonts[0])\n",
    "    ax1.set_ylabel(\"Support\", size=fonts[0])\n",
    "    plt.setp(ax1.get_yticklabels(),\n",
    "             size=fonts[0])\n",
    "    plt.setp(ax2.get_yticklabels(),\n",
    "             size=fonts[0])\n",
    "    \n",
    "    return classification_report(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "confusion_df = pd.read_csv('outputs/biodiscover/03_ept_run_efficientnet_b0/f0/predictions/biodiscover_none/_03_ept_run_efficientnet_b0_f0_240529-0859-5fb9_epoch50_val-loss0.82_last_none.csv')\n",
    "\n",
    "confusion_matrixX(confusion_df['y_true'], confusion_df['y_pred'], 'union')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/04_group_predictions.py \\\n",
    "    --predictions \"outputs/biodiscover/aug_resnet18/f0/predictions/biodiscover_none/_aug_resnet18_f0_240524-1427-0065_epoch50_val-loss1.62_last_none.csv\" \\\n",
    "    --reference_csv \"data/processed/ept/EPT_processed_5splits_Final_species_label.csv\" \\\n",
    "    --reference_target \"Final_species_label\" \\\n",
    "    --fold 0 \\\n",
    "    --reference_group \"individual\" \\\n",
    "    --agg_func \"mode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
