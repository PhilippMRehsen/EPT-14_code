{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (2.4.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (0.19.1)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (2.4.1)\n",
      "Requirement already satisfied: lightning in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (2.3.3)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (0.19.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (1.21.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (1.3.2)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 9)) (0.21.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 10)) (2.0.3)\n",
      "Requirement already satisfied: pillow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 11)) (8.4.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 12)) (1.4.2)\n",
      "Requirement already satisfied: matplotlib==3.7.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 13)) (3.7.3)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 14)) (0.13.2)\n",
      "Requirement already satisfied: bokeh in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 15)) (3.1.1)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 16)) (5.24.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 17)) (4.67.1)\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 18)) (3.1.5)\n",
      "Requirement already satisfied: timm in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 19)) (1.0.12)\n",
      "Requirement already satisfied: albumentations in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 20)) (1.4.18)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from torch->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch->-r requirements.txt (line 1)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch->-r requirements.txt (line 1)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch->-r requirements.txt (line 1)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch->-r requirements.txt (line 1)) (12.1.0.106)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch->-r requirements.txt (line 1)) (3.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch->-r requirements.txt (line 1)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch->-r requirements.txt (line 1)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch->-r requirements.txt (line 1)) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch->-r requirements.txt (line 1)) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version < \"3.13\" in /usr/local/lib/python3.8/dist-packages (from torch->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch->-r requirements.txt (line 1)) (3.16.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch->-r requirements.txt (line 1)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch->-r requirements.txt (line 1)) (12.1.3.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch->-r requirements.txt (line 1)) (1.13.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch->-r requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from torch->-r requirements.txt (line 1)) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch->-r requirements.txt (line 1)) (10.3.2.106)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.8/dist-packages (from lightning->-r requirements.txt (line 4)) (21.2)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from lightning->-r requirements.txt (line 4)) (1.5.2)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.8/dist-packages (from lightning->-r requirements.txt (line 4)) (6.0.2)\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.8/dist-packages (from lightning->-r requirements.txt (line 4)) (2.4.0)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from lightning->-r requirements.txt (line 4)) (0.11.9)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb->-r requirements.txt (line 5)) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.8/dist-packages (from wandb->-r requirements.txt (line 5)) (4.3.6)\n",
      "Requirement already satisfied: eval-type-backport; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from wandb->-r requirements.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from wandb->-r requirements.txt (line 5)) (0.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.12.0; python_version < \"3.9\" and sys_platform == \"linux\" in /usr/local/lib/python3.8/dist-packages (from wandb->-r requirements.txt (line 5)) (3.19.1)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb->-r requirements.txt (line 5)) (2.19.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb->-r requirements.txt (line 5)) (58.5.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb->-r requirements.txt (line 5)) (6.1.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.8/dist-packages (from wandb->-r requirements.txt (line 5)) (1.3.4)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.8/dist-packages (from wandb->-r requirements.txt (line 5)) (2.10.3)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.8/dist-packages (from wandb->-r requirements.txt (line 5)) (8.1.7)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/lib/python3/dist-packages (from wandb->-r requirements.txt (line 5)) (2.22.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->-r requirements.txt (line 8)) (3.5.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.8/dist-packages (from scikit-image->-r requirements.txt (line 9)) (2023.7.10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image->-r requirements.txt (line 9)) (1.4.1)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in /usr/local/lib/python3.8/dist-packages (from scikit-image->-r requirements.txt (line 9)) (0.4)\n",
      "Requirement already satisfied: imageio>=2.27 in /usr/local/lib/python3.8/dist-packages (from scikit-image->-r requirements.txt (line 9)) (2.35.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 10)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 10)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 10)) (2024.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.7.3->-r requirements.txt (line 13)) (4.55.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.7.3->-r requirements.txt (line 13)) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.7.3->-r requirements.txt (line 13)) (1.3.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.7.3->-r requirements.txt (line 13)) (1.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.7.3->-r requirements.txt (line 13)) (2.4.7)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.7.3->-r requirements.txt (line 13)) (5.4.0)\n",
      "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.8/dist-packages (from bokeh->-r requirements.txt (line 15)) (6.1)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.8/dist-packages (from bokeh->-r requirements.txt (line 15)) (2024.9.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from plotly->-r requirements.txt (line 16)) (9.0.0)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.8/dist-packages (from openpyxl->-r requirements.txt (line 18)) (2.0.0)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.8/dist-packages (from timm->-r requirements.txt (line 19)) (0.26.5)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.8/dist-packages (from timm->-r requirements.txt (line 19)) (0.4.5)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.8/dist-packages (from albumentations->-r requirements.txt (line 20)) (4.10.0.84)\n",
      "Requirement already satisfied: albucore==0.0.17 in /usr/local/lib/python3.8/dist-packages (from albumentations->-r requirements.txt (line 20)) (0.0.17)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.8/dist-packages (from nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch->-r requirements.txt (line 1)) (12.6.85)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 5)) (4.0.11)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 5)) (1.14.0)\n",
      "Requirement already satisfied: urllib3>=1.26.11 in /usr/local/lib/python3.8/dist-packages (from sentry-sdk>=2.0.0->wandb->-r requirements.txt (line 5)) (2.2.3)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from sentry-sdk>=2.0.0->wandb->-r requirements.txt (line 5)) (2019.11.28)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from pydantic<3,>=2.6->wandb->-r requirements.txt (line 5)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.8/dist-packages (from pydantic<3,>=2.6->wandb->-r requirements.txt (line 5)) (2.27.1)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=3.2.0; python_version < \"3.10\"->matplotlib==3.7.3->-r requirements.txt (line 13)) (3.6.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 5)) (5.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///tf/notebooks/EPT/1_taxonomist\n",
      "Installing collected packages: taxonomist\n",
      "  Attempting uninstall: taxonomist\n",
      "    Found existing installation: taxonomist 0.0.3\n",
      "    Can't uninstall 'taxonomist'. No files were found to uninstall.\n",
      "  Running setup.py develop for taxonomist\n",
      "Successfully installed taxonomist\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Specimen ID</th>\n",
       "      <th>Sample Name/Number</th>\n",
       "      <th>Species Name</th>\n",
       "      <th>Image File Name</th>\n",
       "      <th>Other Notes</th>\n",
       "      <th>Max Feret Diameter</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Area</th>\n",
       "      <th>Holes</th>\n",
       "      <th>Area+Holes</th>\n",
       "      <th>...</th>\n",
       "      <th>Genus_DNA</th>\n",
       "      <th>Species_DNA</th>\n",
       "      <th>Label_DNA</th>\n",
       "      <th>read count_DNA</th>\n",
       "      <th>individual</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4_F12</td>\n",
       "      <td>4_F12_1</td>\n",
       "      <td>PMR_ExStream22_Main_EPT</td>\n",
       "      <td>1_4_F12_1_2023_12_14-09-51-53-989.PNG</td>\n",
       "      <td>C25_T_mit</td>\n",
       "      <td>352</td>\n",
       "      <td>923</td>\n",
       "      <td>25295</td>\n",
       "      <td>19</td>\n",
       "      <td>25314</td>\n",
       "      <td>...</td>\n",
       "      <td>Goera</td>\n",
       "      <td>Goera pilosa</td>\n",
       "      <td>Goera pilosa</td>\n",
       "      <td>215998.0</td>\n",
       "      <td>4_F12</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>val</td>\n",
       "      <td>train</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4_F12</td>\n",
       "      <td>4_F12_1</td>\n",
       "      <td>PMR_ExStream22_Main_EPT</td>\n",
       "      <td>2_4_F12_1_2023_12_14-09-51-54-000.PNG</td>\n",
       "      <td>C25_T_mit</td>\n",
       "      <td>367</td>\n",
       "      <td>1034</td>\n",
       "      <td>28245</td>\n",
       "      <td>168</td>\n",
       "      <td>28413</td>\n",
       "      <td>...</td>\n",
       "      <td>Goera</td>\n",
       "      <td>Goera pilosa</td>\n",
       "      <td>Goera pilosa</td>\n",
       "      <td>215998.0</td>\n",
       "      <td>4_F12</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>val</td>\n",
       "      <td>train</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4_F12</td>\n",
       "      <td>4_F12_1</td>\n",
       "      <td>PMR_ExStream22_Main_EPT</td>\n",
       "      <td>1_4_F12_1_2023_12_14-09-51-54-013.PNG</td>\n",
       "      <td>C25_T_mit</td>\n",
       "      <td>400</td>\n",
       "      <td>1093</td>\n",
       "      <td>28301</td>\n",
       "      <td>21</td>\n",
       "      <td>28322</td>\n",
       "      <td>...</td>\n",
       "      <td>Goera</td>\n",
       "      <td>Goera pilosa</td>\n",
       "      <td>Goera pilosa</td>\n",
       "      <td>215998.0</td>\n",
       "      <td>4_F12</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>val</td>\n",
       "      <td>train</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4_F12</td>\n",
       "      <td>4_F12_1</td>\n",
       "      <td>PMR_ExStream22_Main_EPT</td>\n",
       "      <td>2_4_F12_1_2023_12_14-09-51-54-025.PNG</td>\n",
       "      <td>C25_T_mit</td>\n",
       "      <td>380</td>\n",
       "      <td>999</td>\n",
       "      <td>29520</td>\n",
       "      <td>43</td>\n",
       "      <td>29563</td>\n",
       "      <td>...</td>\n",
       "      <td>Goera</td>\n",
       "      <td>Goera pilosa</td>\n",
       "      <td>Goera pilosa</td>\n",
       "      <td>215998.0</td>\n",
       "      <td>4_F12</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>val</td>\n",
       "      <td>train</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4_F12</td>\n",
       "      <td>4_F12_2</td>\n",
       "      <td>PMR_ExStream22_Main_EPT</td>\n",
       "      <td>1_4_F12_2_2023_12_14-09-52-19-798.PNG</td>\n",
       "      <td>C25_T_mit</td>\n",
       "      <td>239</td>\n",
       "      <td>645</td>\n",
       "      <td>16395</td>\n",
       "      <td>1</td>\n",
       "      <td>16396</td>\n",
       "      <td>...</td>\n",
       "      <td>Goera</td>\n",
       "      <td>Goera pilosa</td>\n",
       "      <td>Goera pilosa</td>\n",
       "      <td>215998.0</td>\n",
       "      <td>4_F12</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>val</td>\n",
       "      <td>train</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Specimen ID Sample Name/Number             Species Name  \\\n",
       "0       4_F12            4_F12_1  PMR_ExStream22_Main_EPT   \n",
       "1       4_F12            4_F12_1  PMR_ExStream22_Main_EPT   \n",
       "2       4_F12            4_F12_1  PMR_ExStream22_Main_EPT   \n",
       "3       4_F12            4_F12_1  PMR_ExStream22_Main_EPT   \n",
       "4       4_F12            4_F12_2  PMR_ExStream22_Main_EPT   \n",
       "\n",
       "                         Image File Name Other Notes  Max Feret Diameter  \\\n",
       "0  1_4_F12_1_2023_12_14-09-51-53-989.PNG   C25_T_mit                 352   \n",
       "1  2_4_F12_1_2023_12_14-09-51-54-000.PNG   C25_T_mit                 367   \n",
       "2  1_4_F12_1_2023_12_14-09-51-54-013.PNG   C25_T_mit                 400   \n",
       "3  2_4_F12_1_2023_12_14-09-51-54-025.PNG   C25_T_mit                 380   \n",
       "4  1_4_F12_2_2023_12_14-09-52-19-798.PNG   C25_T_mit                 239   \n",
       "\n",
       "   Perimeter   Area  Holes  Area+Holes  ...  Genus_DNA   Species_DNA  \\\n",
       "0        923  25295     19       25314  ...      Goera  Goera pilosa   \n",
       "1       1034  28245    168       28413  ...      Goera  Goera pilosa   \n",
       "2       1093  28301     21       28322  ...      Goera  Goera pilosa   \n",
       "3        999  29520     43       29563  ...      Goera  Goera pilosa   \n",
       "4        645  16395      1       16396  ...      Goera  Goera pilosa   \n",
       "\n",
       "      Label_DNA  read count_DNA individual      0      1    2      3     4  \n",
       "0  Goera pilosa        215998.0      4_F12  train  train  val  train  test  \n",
       "1  Goera pilosa        215998.0      4_F12  train  train  val  train  test  \n",
       "2  Goera pilosa        215998.0      4_F12  train  train  val  train  test  \n",
       "3  Goera pilosa        215998.0      4_F12  train  train  val  train  test  \n",
       "4  Goera pilosa        215998.0      4_F12  train  train  val  train  test  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/processed/EPT-14/01_EPT_processed_5splits_Species_DNA.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training pre-trained CNNs with frozen weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def list_files_with_prefix(directory, prefix, model):\n",
    "    # Define the pattern for the files to search\n",
    "    pattern = os.path.join(directory, f\"{prefix}*\")\n",
    "    \n",
    "    # Use glob to find files matching the pattern\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    return files\n",
    "\n",
    "def list_ckpts(fold_directory):\n",
    "    # Define the pattern for the files to search    \n",
    "    pattern = os.path.join(fold_directory, '*.ckpt')\n",
    "    \n",
    "    # Use glob to find files matching the pattern, with recursive search\n",
    "    ckpt_files = glob.glob(pattern)\n",
    "    \n",
    "    filtered_files = [f for f in ckpt_files if model in f if '_241202' in f if not f.endswith('_last.ckpt')]     # Filter out files ending with '_last.ckpt' and only return desired model\n",
    "    \n",
    "    # Return the first file from the filtered list or None if the list is empty\n",
    "    return filtered_files\n",
    "\n",
    "def extend_fold_dict(directory):\n",
    "    for subfolder in directory:\n",
    "        folds = [f.path for f in os.scandir(subfolder) if f.is_dir() and not f.name.startswith('.')]\n",
    "        # Filter out dotfiles such as '.ipynb_checkpoint' files \n",
    "        for fold in folds:\n",
    "            fold_dict[int(fold[-1])].extend(list_ckpts(fold))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for the best model checkpoint in the directory\n",
    "directory = 'outputs/biodiscover/'\n",
    "prefix = 'fV1-EPT-14'\n",
    "model = 'efficientnet_b0'\n",
    "\n",
    "    \n",
    "fold_dict={}\n",
    "for number in range(0,5):\n",
    "    fold_dict[number] = []\n",
    "\n",
    "# List the files\n",
    "setting_folders = list_files_with_prefix(directory, prefix, model)\n",
    "\n",
    "extend_fold_dict(setting_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['outputs/biodiscover/fV1-EPT-14-Family_DNA_efficientnet_b0/f0/fV1-EPT-14-Family_DNA_efficientnet_b0_f0_241202-0953-608e_epoch05_val-loss0.45.ckpt',\n",
       "  'outputs/biodiscover/fV1-EPT-14-Genus_DNA_efficientnet_b0/f0/fV1-EPT-14-Genus_DNA_efficientnet_b0_f0_241202-1006-96fd_epoch05_val-loss0.47.ckpt',\n",
       "  'outputs/biodiscover/fV1-EPT-14-Order_DNA_efficientnet_b0/f0/fV1-EPT-14-Order_DNA_efficientnet_b0_f0_241202-1020-7554_epoch03_val-loss0.48.ckpt',\n",
       "  'outputs/biodiscover/fV1-EPT-14-Species_DNA_efficientnet_b0/f0/fV1-EPT-14-Species_DNA_efficientnet_b0_f0_241202-1033-49e6_epoch00_val-loss0.45.ckpt'],\n",
       " 1: ['outputs/biodiscover/fV1-EPT-14-Family_DNA_efficientnet_b0/f1/fV1-EPT-14-Family_DNA_efficientnet_b0_f1_241202-1047-5446_epoch03_val-loss0.49.ckpt',\n",
       "  'outputs/biodiscover/fV1-EPT-14-Genus_DNA_efficientnet_b0/f1/fV1-EPT-14-Genus_DNA_efficientnet_b0_f1_241202-1100-da43_epoch02_val-loss0.47.ckpt',\n",
       "  'outputs/biodiscover/fV1-EPT-14-Order_DNA_efficientnet_b0/f1/fV1-EPT-14-Order_DNA_efficientnet_b0_f1_241202-1113-0ac9_epoch08_val-loss0.47.ckpt',\n",
       "  'outputs/biodiscover/fV1-EPT-14-Species_DNA_efficientnet_b0/f1/fV1-EPT-14-Species_DNA_efficientnet_b0_f1_241202-1127-8b76_epoch04_val-loss0.47.ckpt'],\n",
       " 2: ['outputs/biodiscover/fV1-EPT-14-Family_DNA_efficientnet_b0/f2/fV1-EPT-14-Family_DNA_efficientnet_b0_f2_241202-1140-f938_epoch01_val-loss0.36.ckpt',\n",
       "  'outputs/biodiscover/fV1-EPT-14-Genus_DNA_efficientnet_b0/f2/fV1-EPT-14-Genus_DNA_efficientnet_b0_f2_241202-1154-1873_epoch08_val-loss0.37.ckpt',\n",
       "  'outputs/biodiscover/fV1-EPT-14-Order_DNA_efficientnet_b0/f2/fV1-EPT-14-Order_DNA_efficientnet_b0_f2_241202-1207-c731_epoch05_val-loss0.37.ckpt',\n",
       "  'outputs/biodiscover/fV1-EPT-14-Species_DNA_efficientnet_b0/f2/fV1-EPT-14-Species_DNA_efficientnet_b0_f2_241202-1220-b526_epoch03_val-loss0.35.ckpt'],\n",
       " 3: ['outputs/biodiscover/fV1-EPT-14-Family_DNA_efficientnet_b0/f3/fV1-EPT-14-Family_DNA_efficientnet_b0_f3_241202-1234-c90e_epoch02_val-loss0.52.ckpt',\n",
       "  'outputs/biodiscover/fV1-EPT-14-Genus_DNA_efficientnet_b0/f3/fV1-EPT-14-Genus_DNA_efficientnet_b0_f3_241202-1247-ce68_epoch06_val-loss0.49.ckpt',\n",
       "  'outputs/biodiscover/fV1-EPT-14-Order_DNA_efficientnet_b0/f3/fV1-EPT-14-Order_DNA_efficientnet_b0_f3_241202-1301-3289_epoch02_val-loss0.51.ckpt',\n",
       "  'outputs/biodiscover/fV1-EPT-14-Species_DNA_efficientnet_b0/f3/fV1-EPT-14-Species_DNA_efficientnet_b0_f3_241202-1314-46a7_epoch00_val-loss0.49.ckpt'],\n",
       " 4: ['outputs/biodiscover/fV1-EPT-14-Family_DNA_efficientnet_b0/f4/fV1-EPT-14-Family_DNA_efficientnet_b0_f4_241202-1328-ecef_epoch03_val-loss0.45.ckpt',\n",
       "  'outputs/biodiscover/fV1-EPT-14-Genus_DNA_efficientnet_b0/f4/fV1-EPT-14-Genus_DNA_efficientnet_b0_f4_241202-1341-2ef2_epoch01_val-loss0.44.ckpt',\n",
       "  'outputs/biodiscover/fV1-EPT-14-Order_DNA_efficientnet_b0/f4/fV1-EPT-14-Order_DNA_efficientnet_b0_f4_241202-1354-3df2_epoch06_val-loss0.44.ckpt',\n",
       "  'outputs/biodiscover/fV1-EPT-14-Species_DNA_efficientnet_b0/f4/fV1-EPT-14-Species_DNA_efficientnet_b0_f4_241202-1407-df94_epoch01_val-loss0.43.ckpt']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login() #581221dff0063a897a4eff9d285fa871a2c8e7f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01_ept-14-biomass was used to determine the learning rate for 'Log_weights'\n",
    "# 02_ept-14-biomass was used to train for 100 epochs on 'Log_weights'\n",
    "# 03_ept-14-biomass was used to pre-train for 100 epochs on 'Species_DNA', 'Genus_DNA', 'Familiy_DNA', 'Order_DNA'\n",
    "\n",
    "# f'02_EPT-14-biomass-{label}-{model}' was used to pre-train for 100 epochs on 'Species_DNA', 'Genus_DNA', 'Familiy_DNA', 'Order_DNA' wit early stopping = True and 20 epochs patience\n",
    "\n",
    "# Train biomass estimation on all 5 folds of previously created splits.\n",
    "for fold, ls in fold_dict.items():\n",
    "    for file in ls:\n",
    "        if 'Species_DNA' in file:\n",
    "            label = 'Species_DNA' # only used for naming, the train label is log_weights\n",
    "        if 'Genus_DNA' in file:\n",
    "            label = 'Genus_DNA' # only used for naming, the train label is log_weights\n",
    "        if 'Family_DNA' in file:\n",
    "            label = 'Family_DNA' # only used for naming, the train label is log_weights\n",
    "        if 'Order_DNA' in file:\n",
    "            label = 'Order_DNA' # only used for naming, the train label is log_weights\n",
    "        !python scripts/02_train.py \\\n",
    "            --data_folder \"../10_images/\" \\\n",
    "            --dataset_config \"conf/user_datasets.py\" \\\n",
    "            --dataset_name \"biodiscover\" \\\n",
    "            --csv_path \"data/processed/EPT-14/01_EPT_processed_5splits_Species_DNA.csv\" \\\n",
    "            --ckpt_path {file} \\\n",
    "            --label \"Log_weights\" \\\n",
    "            --fold {fold} \\\n",
    "            --class_map 'none' \\\n",
    "            --imsize 224 \\\n",
    "            --batch_size 256 \\\n",
    "            --aug 'flips-rotate-keep-aspect' \\\n",
    "            --load_to_memory 'False' \\\n",
    "            --model 'efficientnet_b0' \\\n",
    "            --freeze_base 'True' \\\n",
    "            --pretrained 'True' \\\n",
    "            --opt 'adamw' \\\n",
    "            --max_epochs 10 \\\n",
    "            --min_epochs 0 \\\n",
    "            --early_stopping 'False' \\\n",
    "            --early_stopping_patience 10 \\\n",
    "            --criterion 'l1' \\\n",
    "            --lr 0.0001 \\\n",
    "            --auto_lr 'True' \\\n",
    "            --log_dir 'V1-EPT-14' \\\n",
    "            --out_folder 'outputs' \\\n",
    "            --out_prefix f'V1-EPT-14-{label}' \\\n",
    "            --deterministic 'True' \\\n",
    "            --precision '16-mixed' \\\n",
    "            --resume 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def list_files_with_prefix(directory, prefix):\n",
    "    # Define the pattern for the files to search\n",
    "    pattern = os.path.join(directory, f\"{prefix}*\")\n",
    "    \n",
    "    # Use glob to find files matching the pattern\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    return files\n",
    "\n",
    "def list_ckpts(fold_directory):\n",
    "    # Define the pattern for the files to search    \n",
    "    pattern = os.path.join(fold_directory, '*.ckpt')\n",
    "    \n",
    "    # Use glob to find files matching the pattern, with recursive search\n",
    "    ckpt_files = glob.glob(pattern)\n",
    "    \n",
    "    filtered_files = [f for f in ckpt_files if '_241202' in f and not f.endswith('_last.ckpt')]\n",
    "     # Filter out files ending with '_last.ckpt' and the classification models (pre-training)\n",
    "    \n",
    "    # Return the first file from the filtered list or None if the list is empty\n",
    "    return filtered_files\n",
    "\n",
    "def extend_fold_dict(directory):\n",
    "    for subfolder in directory:\n",
    "        folds = [f.path for f in os.scandir(subfolder) if f.is_dir()]\n",
    "        for fold in folds:\n",
    "            fold_dict[int(fold[-1])].extend(list_ckpts(fold))\n",
    "        \n",
    "directory = 'outputs/biodiscover/'\n",
    "prefix = 'fV1-EPT-14'\n",
    "\n",
    "    \n",
    "fold_dict={}\n",
    "for number in range(0,5):\n",
    "    fold_dict[number] = []\n",
    "\n",
    "# List the files in fold_dict\n",
    "setting_folders = list_files_with_prefix(directory, prefix)\n",
    "\n",
    "extend_fold_dict(setting_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, ls in fold_dict.items():\n",
    "    for file in ls:\n",
    "        # Automatic predictiongs using the first (best) model checkpoint in the directory.\n",
    "        !python scripts/03_predict.py \\\n",
    "            --data_folder \"../10_images/\" \\\n",
    "            --dataset_config \"conf/user_datasets.py\" \\\n",
    "            --dataset_name \"biodiscover\" \\\n",
    "            --csv_path \"data/processed/EPT-14/01_EPT_processed_5splits_Species_DNA.csv\" \\\n",
    "            --label \"Log_weights\" \\\n",
    "            --fold {fold} \\\n",
    "            --class_map 'none' \\\n",
    "            --imsize 224 \\\n",
    "            --batch_size 512 \\\n",
    "            --aug 'none' \\\n",
    "            --out_folder 'outputs' \\\n",
    "            --tta 'False' \\\n",
    "            --out_prefix 'fV1-EPT-14' \\\n",
    "            --ckpt_path {file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "def list_files_with_prefix(directory, prefix):\n",
    "    pattern = os.path.join(directory, f\"{prefix}*\")\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    return files\n",
    "\n",
    "def list_csvs(directory): \n",
    "    pattern = os.path.join(directory, 'predictions', 'biodiscover_none', '*.csv')\n",
    "    csv_files = glob.glob(pattern)\n",
    "    csv_files = [f for f in csv_files if '_241202' in f]\n",
    "    return csv_files\n",
    "\n",
    "def extend_run_dict_csv(directory):\n",
    "    folds = [f.path for f in os.scandir(directory) if f.is_dir()]\n",
    "    for fold in folds:\n",
    "        run_dict_csv[directory].extend(list_csvs(fold))\n",
    "        \n",
    "directory = 'outputs/biodiscover/'\n",
    "prefix = 'fV1-EPT-14'        \n",
    "        \n",
    "run_dict_csv={}\n",
    "for run in list_files_with_prefix(directory, prefix):\n",
    "    run_dict_csv[run] = []\n",
    "    \n",
    "setting_folders = list_files_with_prefix(directory, prefix)\n",
    "\n",
    "for setting in setting_folders:\n",
    "    extend_run_dict_csv(setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dict_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "ref = pd.read_csv(\"data/processed/EPT-14/01_EPT_processed_5splits_Species_DNA.csv\")\n",
    "\n",
    "for run, fold_ls in run_dict_csv.items():\n",
    "    print('joining '+ run)\n",
    "    df_f0 = pd.read_csv(fold_ls[0])\n",
    "    df_f1 = pd.read_csv(fold_ls[1])\n",
    "    df_f2 = pd.read_csv(fold_ls[2])\n",
    "    df_f3 = pd.read_csv(fold_ls[3])\n",
    "    df_f4 = pd.read_csv(fold_ls[4])\n",
    "    df = pd.concat([df_f0, df_f1, df_f2, df_f3, df_f4])\n",
    "\n",
    "    if 'Species_DNA' in run:\n",
    "        label = 'Species_DNA' # only used for naming, the train label is log_weights\n",
    "    if 'Genus_DNA' in run:\n",
    "        label = 'Genus_DNA' # only used for naming, the train label is log_weights\n",
    "    if 'Family_DNA' in run:\n",
    "        label = 'Family_DNA' # only used for naming, the train label is log_weights\n",
    "    if 'Order_DNA' in run:\n",
    "        label = 'Order_DNA' # only used for naming, the train label is log_weights\n",
    "        \n",
    "    # Get just the filename from the full path\n",
    "    df[\"fname\"] = df[\"fname\"].apply(lambda x: Path(x).name)\n",
    "    preds = df[[\"fname\", \"y_pred\"]].set_index(\"fname\")\n",
    "\n",
    "    # Join the dataframes. 'inner' to keep only the rows that are in both dataframes\n",
    "    results_df = ref.join(preds, how=\"inner\", on=\"Image File Name\")  \n",
    "\n",
    "    # Choose wanted columns\n",
    "    results_df = results_df[['Log_weights', \"y_pred\", \"individual\"]]\n",
    "\n",
    "    # Grouped by individual\n",
    "    results_df_grouped = results_df.groupby([\"individual\"]).agg(lambda x: pd.Series.mode(x)[0])\n",
    "\n",
    "    results_df_grouped.to_csv(f'{run}/fV1-EPT-14_{label}_biomass.csv')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training pre-trained CNNs with unfrozen weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def list_files_with_prefix(directory, prefix, model):\n",
    "    # Define the pattern for the files to search\n",
    "    pattern = os.path.join(directory, f\"{prefix}*\")\n",
    "    \n",
    "    # Use glob to find files matching the pattern\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    return files\n",
    "\n",
    "def list_ckpts(fold_directory):\n",
    "    # Define the pattern for the files to search    \n",
    "    pattern = os.path.join(fold_directory, '*.ckpt')\n",
    "    \n",
    "    # Use glob to find files matching the pattern, with recursive search\n",
    "    ckpt_files = glob.glob(pattern)\n",
    "    \n",
    "    filtered_files = [f for f in ckpt_files if '_241202' in f and not f.endswith('_last.ckpt')]\n",
    "    # Filter out files ending with '_last.ckpt' and the classification models (pre-training)\n",
    "    \n",
    "    # Return the first file from the filtered list or None if the list is empty\n",
    "    return filtered_files\n",
    "\n",
    "def extend_fold_dict(directory):\n",
    "    for subfolder in directory:\n",
    "        folds = [f.path for f in os.scandir(subfolder) if f.is_dir() and not f.name.startswith('.')]\n",
    "        # Filter out dotfiles such as '.ipynb_checkpoint' files \n",
    "        for fold in folds:\n",
    "            fold_dict[int(fold[-1])].extend(list_ckpts(fold))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for the best model checkpoint in the directory\n",
    "directory = 'outputs/biodiscover/'\n",
    "prefix = 'fV1-EPT-14'\n",
    "model = 'efficientnet_b0'\n",
    "\n",
    "    \n",
    "fold_dict={}\n",
    "for number in range(0,5):\n",
    "    fold_dict[number] = []\n",
    "\n",
    "# List the files\n",
    "setting_folders = list_files_with_prefix(directory, prefix, model)\n",
    "\n",
    "extend_fold_dict(setting_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login() #581221dff0063a897a4eff9d285fa871a2c8e7f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01_ept-14-biomass was used to determine the learning rate for 'Log_weights'\n",
    "# 02_ept-14-biomass was used to train for 100 epochs on 'Log_weights'\n",
    "# 03_ept-14-biomass was used to pre-train for 100 epochs on 'Species_DNA', 'Genus_DNA', 'Familiy_DNA', 'Order_DNA'\n",
    "\n",
    "# f'02_EPT-14-biomass-{label}-{model}' was used to pre-train for 100 epochs on 'Species_DNA', 'Genus_DNA', 'Familiy_DNA', 'Order_DNA' wit early stopping = True and 20 epochs patience\n",
    "\n",
    "# Train biomass estimation on all 5 folds of previously created splits.\n",
    "for fold, ls in fold_dict.items():\n",
    "    for file in ls:\n",
    "        if 'Species_DNA' in file:\n",
    "            label = 'Species_DNA' # only used for naming, the train label is log_weights\n",
    "        if 'Genus_DNA' in file:\n",
    "            label = 'Genus_DNA' # only used for naming, the train label is log_weights\n",
    "        if 'Family_DNA' in file:\n",
    "            label = 'Family_DNA' # only used for naming, the train label is log_weights\n",
    "        if 'Order_DNA' in file:\n",
    "            label = 'Order_DNA' # only used for naming, the train label is log_weights\n",
    "        !python scripts/02_train.py \\\n",
    "            --data_folder \"../10_images/\" \\\n",
    "            --dataset_config \"conf/user_datasets.py\" \\\n",
    "            --dataset_name \"biodiscover\" \\\n",
    "            --csv_path \"data/processed/EPT-14/01_EPT_processed_5splits_Species_DNA.csv\" \\\n",
    "            --ckpt_path {file} \\\n",
    "            --label \"Log_weights\" \\\n",
    "            --fold {fold} \\\n",
    "            --class_map 'none' \\\n",
    "            --imsize 224 \\\n",
    "            --batch_size 256 \\\n",
    "            --aug 'flips-rotate-keep-aspect' \\\n",
    "            --load_to_memory 'False' \\\n",
    "            --model 'efficientnet_b0' \\\n",
    "            --freeze_base 'False' \\\n",
    "            --pretrained 'True' \\\n",
    "            --opt 'adamw' \\\n",
    "            --max_epochs 10 \\\n",
    "            --min_epochs 0 \\\n",
    "            --early_stopping 'False' \\\n",
    "            --early_stopping_patience 10 \\\n",
    "            --criterion 'l1' \\\n",
    "            --lr 0.0001 \\\n",
    "            --auto_lr 'True' \\\n",
    "            --log_dir 'V1-EPT-14-unfrozen' \\\n",
    "            --out_folder 'outputs' \\\n",
    "            --out_prefix f'V1-EPT-14-{label}-unfrozen' \\\n",
    "            --deterministic 'True' \\\n",
    "            --precision '16-mixed' \\\n",
    "            --resume 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def list_files_with_prefix(directory, prefix):\n",
    "    # Define the pattern for the files to search\n",
    "    pattern = os.path.join(directory, f\"{prefix}*\")\n",
    "    \n",
    "    # Use glob to find files matching the pattern\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    return files\n",
    "\n",
    "def list_ckpts(fold_directory):\n",
    "    # Define the pattern for the files to search    \n",
    "    pattern = os.path.join(fold_directory, '*.ckpt')\n",
    "    \n",
    "    # Use glob to find files matching the pattern, with recursive search\n",
    "    ckpt_files = glob.glob(pattern)\n",
    "    \n",
    "    filtered_files = [f for f in ckpt_files if 'unfrozen' in f and not f.endswith('_last.ckpt')]\n",
    "     # Filter out files ending with '_last.ckpt' and the classification models (pre-training)\n",
    "    \n",
    "    # Return the first file from the filtered list or None if the list is empty\n",
    "    return filtered_files\n",
    "\n",
    "def extend_fold_dict(directory):\n",
    "    for subfolder in directory:\n",
    "        folds = [f.path for f in os.scandir(subfolder) if f.is_dir() and not f.name.startswith('.')]\n",
    "        for fold in folds:\n",
    "            fold_dict[int(fold[-1])].extend(list_ckpts(fold))\n",
    "        \n",
    "directory = 'outputs/biodiscover/'\n",
    "prefix = 'fV1-EPT-14'\n",
    "\n",
    "    \n",
    "fold_dict={}\n",
    "for number in range(0,5):\n",
    "    fold_dict[number] = []\n",
    "\n",
    "# List the files in fold_dict\n",
    "setting_folders = list_files_with_prefix(directory, prefix)\n",
    "\n",
    "extend_fold_dict(setting_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, ls in fold_dict.items():\n",
    "    for file in ls:\n",
    "        # Automatic predictiongs using the first (best) model checkpoint in the directory.\n",
    "        !python scripts/03_predict.py \\\n",
    "            --data_folder \"../10_images/\" \\\n",
    "            --dataset_config \"conf/user_datasets.py\" \\\n",
    "            --dataset_name \"biodiscover\" \\\n",
    "            --csv_path \"data/processed/EPT-14/01_EPT_processed_5splits_Species_DNA.csv\" \\\n",
    "            --label \"Log_weights\" \\\n",
    "            --fold {fold} \\\n",
    "            --class_map 'none' \\\n",
    "            --imsize 224 \\\n",
    "            --batch_size 512 \\\n",
    "            --aug 'none' \\\n",
    "            --out_folder 'outputs' \\\n",
    "            --tta 'False' \\\n",
    "            --out_prefix 'fV1-EPT-14' \\\n",
    "            --ckpt_path {file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "def list_files_with_prefix(directory, prefix):\n",
    "    pattern = os.path.join(directory, f\"{prefix}*\")\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    return files\n",
    "\n",
    "def list_csvs(directory): \n",
    "    pattern = os.path.join(directory, 'predictions', 'biodiscover_none', '*.csv')\n",
    "    csv_files = glob.glob(pattern)\n",
    "    csv_files = [f for f in csv_files if 'unfrozen' in f]\n",
    "    return csv_files\n",
    "\n",
    "def extend_run_dict_csv(directory):\n",
    "    folds = [f.path for f in os.scandir(directory) if f.is_dir()]\n",
    "    for fold in folds:\n",
    "        run_dict_csv[directory].extend(list_csvs(fold))\n",
    "        \n",
    "directory = 'outputs/biodiscover/'\n",
    "prefix = 'fV1-EPT-14'        \n",
    "        \n",
    "run_dict_csv={}\n",
    "for run in list_files_with_prefix(directory, prefix):\n",
    "    run_dict_csv[run] = []\n",
    "    \n",
    "setting_folders = list_files_with_prefix(directory, prefix)\n",
    "\n",
    "for setting in setting_folders:\n",
    "    extend_run_dict_csv(setting)\n",
    "    \n",
    "run_dict_csv = {k: v for k, v in run_dict_csv.items() if 'unfrozen' in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dict_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "ref = pd.read_csv(\"data/processed/EPT-14/01_EPT_processed_5splits_Species_DNA.csv\")\n",
    "\n",
    "for run, fold_ls in run_dict_csv.items():\n",
    "    print('joining '+ run)\n",
    "    df_f0 = pd.read_csv(fold_ls[0])\n",
    "    df_f1 = pd.read_csv(fold_ls[1])\n",
    "    df_f2 = pd.read_csv(fold_ls[2])\n",
    "    df_f3 = pd.read_csv(fold_ls[3])\n",
    "    df_f4 = pd.read_csv(fold_ls[4])\n",
    "    df = pd.concat([df_f0, df_f1, df_f2, df_f3, df_f4])\n",
    "\n",
    "    if 'Species_DNA' in run:\n",
    "        label = 'Species_DNA' # only used for naming, the train label is log_weights\n",
    "    if 'Genus_DNA' in run:\n",
    "        label = 'Genus_DNA' # only used for naming, the train label is log_weights\n",
    "    if 'Family_DNA' in run:\n",
    "        label = 'Family_DNA' # only used for naming, the train label is log_weights\n",
    "    if 'Order_DNA' in run:\n",
    "        label = 'Order_DNA' # only used for naming, the train label is log_weights\n",
    "        \n",
    "    # Get just the filename from the full path\n",
    "    df[\"fname\"] = df[\"fname\"].apply(lambda x: Path(x).name)\n",
    "    preds = df[[\"fname\", \"y_pred\"]].set_index(\"fname\")\n",
    "\n",
    "    # Join the dataframes. 'inner' to keep only the rows that are in both dataframes\n",
    "    results_df = ref.join(preds, how=\"inner\", on=\"Image File Name\")  \n",
    "\n",
    "    # Choose wanted columns\n",
    "    results_df = results_df[['Log_weights', \"y_pred\", \"individual\"]]\n",
    "\n",
    "    # Grouped by individual\n",
    "    results_df_grouped = results_df.groupby([\"individual\"]).apply(lambda group: group[(group >= group.quantile(0.05)) & (group <= group.quantile(0.95))].mean())\n",
    "\n",
    "    results_df_grouped.to_csv(f'{run}/fV1-EPT-14_{label}-unfrozen_biomass.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Until here, the code works fine. The following parts are still more or less experimental and for sure not cleaned up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "confusion_df = pd.read_csv('outputs/biodiscover/02_ept-14-biomass_efficientnet_b0/01_EPT_processed_5splits_Species_DNA_grouped_predictions.csv')\n",
    "\n",
    "confusion_matrixX(confusion_df['y_true'], confusion_df['y_pred'], 'union')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
